<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Aditi Asati</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Aditi Asati</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 08 Jun 2020 08:06:25 +0600</lastBuildDate><atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" /><item>
      <title>How to Kernelize the Ridge Regression Algorithm</title>
      <link>http://localhost:1313/posts/kernel_ridge_regression/</link>
      <pubDate>Mon, 08 Jun 2020 08:06:25 +0600</pubDate>
      
      <guid>http://localhost:1313/posts/kernel_ridge_regression/</guid>
      <description>How to kernelize the Ridge Regression Algorithm Recall that the optimization problem of ridge regression in feature space is given by:
$$ \min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^{n} \left( Y_i - \langle w, \Phi(X_i) \rangle \right)^2 + \lambda \lVert w \rVert_2^2$$
We want to add a non-linear component to ridge regression. Hence, we will kernelize the algorithm by using the following result.
Representer theorem: Let $\mathcal{X}$ and $\mathcal{Y}$ be the input space and output space respectively.</description>
    </item>
    
    
  </channel>
</rss>
