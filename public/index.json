[{"categories":null,"contents":"How to kernelize the Ridge Regression Algorithm Recall that the optimization problem of ridge regression in feature space is given by:\n$$ \\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum_{i=1}^{n} \\left( Y_i - \\langle w, \\Phi(X_i) \\rangle \\right)^2 + \\lambda \\lVert w \\rVert_2^2$$\nWe want to add a non-linear component to ridge regression. Hence, we will kernelize the algorithm by using the following result.\nRepresenter theorem: Let $\\mathcal{X}$ and $\\mathcal{Y}$ be the input space and output space respectively. Let $k : \\mathcal{X} \\times \\mathcal{X} \\rightarrow\\mathbb{R}$ be a kernel, and let $\\mathcal{H}$ be the corresponding RKHS. Given a training set $(X_i, Y_i){i=1, \\ldots, n} \\subset \\mathcal{X} \\times \\mathcal{Y}$ and classifier $f_w(x) := \\langle w, \\Phi(x) \\rangle{\\mathcal{H}}$, let $R_n$ denote the empirical risk of the classifier in relation to a loss function $l$, and $\\Omega : [0, \\infty[ \\rightarrow \\mathbb{R}$, which is a strictly monotonically increasing function. Consider the following regularized risk minimization problem:\n$$\\min_{w \\in \\mathcal{H}} \\left( R_n(w) + \\lambda \\Omega(|w|_{\\mathcal{H}}) \\right)$$\nThen, the theorem states that the optimal solution of the problem always exists and is given as\n$${w^* = \\sum_{i=1}^{n} \\alpha_i k(X_i, \\cdot)}$$\nFrom the representer theorem, the solution $w$ of problem can be expressed as a linear combination of input points in the feature space:\n$$w = \\sum_{j=1}^n \\alpha_j\\Phi(X_j)$$\nSubstituting $w$ in the above ridge regression problem yields the following optimization problem:\n$$\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{n} \\lVert Y - K\\alpha \\rVert_2^2 + \\lambda \\alpha^T K \\alpha$$\nwhere $K$ is the kernel matrix defined. The solution can be derived analytically and is given by\n$${\\alpha = (n\\lambda I + K)^{-1}Y}$$\nWe can also calculate the prediction for an unseen point just using kernels as\n$$f(x) =\\sum_{j=1}^n \\alpha_jk(X_j,x)$$\npip install scikit-learn ","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/kernel_ridge_regression/","summary":"How to kernelize the Ridge Regression Algorithm Recall that the optimization problem of ridge regression in feature space is given by:\n$$ \\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum_{i=1}^{n} \\left( Y_i - \\langle w, \\Phi(X_i) \\rangle \\right)^2 + \\lambda \\lVert w \\rVert_2^2$$\nWe want to add a non-linear component to ridge regression. Hence, we will kernelize the algorithm by using the following result.\nRepresenter theorem: Let $\\mathcal{X}$ and $\\mathcal{Y}$ be the input space and output space respectively.","tags":null,"title":"How to Kernelize the Ridge Regression Algorithm"}]